{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6d8a0e",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faker in c:\\users\\tijn\\documents\\vsc\\data engineering projects\\pyspark_regex_practice\\.env\\lib\\site-packages (from -r requirements.txt (line 1)) (37.1.0)\n",
      "Requirement already satisfied: pyspark in c:\\users\\tijn\\documents\\vsc\\data engineering projects\\pyspark_regex_practice\\.env\\lib\\site-packages (from -r requirements.txt (line 2)) (3.5.5)\n",
      "Requirement already satisfied: tzdata in c:\\users\\tijn\\documents\\vsc\\data engineering projects\\pyspark_regex_practice\\.env\\lib\\site-packages (from faker->-r requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\tijn\\documents\\vsc\\data engineering projects\\pyspark_regex_practice\\.env\\lib\\site-packages (from pyspark->-r requirements.txt (line 2)) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-cache-dir -r requirements.txt\n",
    "!python Create_synth_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9834ca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "logfile = \"logfiles.log\"\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LogFileAnalysis\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2d6ad04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function scan_csv in module polars.io.csv.functions:\n",
      "\n",
      "scan_csv(source: 'str | Path | IO[str] | IO[bytes] | bytes | list[str] | list[Path] | list[IO[str]] | list[IO[bytes]] | list[bytes]', *, has_header: 'bool' = True, separator: 'str' = ',', comment_prefix: 'str | None' = None, quote_char: 'str | None' = '\"', skip_rows: 'int' = 0, skip_lines: 'int' = 0, schema: 'SchemaDict | None' = None, schema_overrides: 'SchemaDict | Sequence[PolarsDataType] | None' = None, null_values: 'str | Sequence[str] | dict[str, str] | None' = None, missing_utf8_is_empty_string: 'bool' = False, ignore_errors: 'bool' = False, cache: 'bool' = True, with_column_names: 'Callable[[list[str]], list[str]] | None' = None, infer_schema: 'bool' = True, infer_schema_length: 'int | None' = 100, n_rows: 'int | None' = None, encoding: 'CsvEncoding' = 'utf8', low_memory: 'bool' = False, rechunk: 'bool' = False, skip_rows_after_header: 'int' = 0, row_index_name: 'str | None' = None, row_index_offset: 'int' = 0, try_parse_dates: 'bool' = False, eol_char: 'str' = '\\n', new_columns: 'Sequence[str] | None' = None, raise_if_empty: 'bool' = True, truncate_ragged_lines: 'bool' = False, decimal_comma: 'bool' = False, glob: 'bool' = True, storage_options: 'dict[str, Any] | None' = None, credential_provider: \"CredentialProviderFunction | Literal['auto'] | None\" = 'auto', retries: 'int' = 2, file_cache_ttl: 'int | None' = None, include_file_paths: 'str | None' = None) -> 'LazyFrame'\n",
      "    Lazily read from a CSV file or multiple files via glob patterns.\n",
      "    \n",
      "    This allows the query optimizer to push down predicates and\n",
      "    projections to the scan level, thereby potentially reducing\n",
      "    memory overhead.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    source\n",
      "        Path(s) to a file or directory\n",
      "        When needing to authenticate for scanning cloud locations, see the\n",
      "        `storage_options` parameter.\n",
      "    has_header\n",
      "        Indicate if the first row of the dataset is a header or not. If set to False,\n",
      "        column names will be autogenerated in the following format: `column_x`, with\n",
      "        `x` being an enumeration over every column in the dataset, starting at 1.\n",
      "    separator\n",
      "        Single byte character to use as separator in the file.\n",
      "    comment_prefix\n",
      "        A string used to indicate the start of a comment line. Comment lines are skipped\n",
      "        during parsing. Common examples of comment prefixes are `#` and `//`.\n",
      "    quote_char\n",
      "        Single byte character used for csv quoting, default = `\"`.\n",
      "        Set to None to turn off special handling and escaping of quotes.\n",
      "    skip_rows\n",
      "        Start reading after ``skip_rows`` rows. The header will be parsed at this\n",
      "        offset. Note that we respect CSV escaping/comments when skipping rows.\n",
      "        If you want to skip by newline char only, use `skip_lines`.\n",
      "    skip_lines\n",
      "        Start reading after `skip_lines` lines. The header will be parsed at this\n",
      "        offset. Note that CSV escaping will not be respected when skipping lines.\n",
      "        If you want to skip valid CSV rows, use ``skip_rows``.\n",
      "    schema\n",
      "        Provide the schema. This means that polars doesn't do schema inference.\n",
      "        This argument expects the complete schema, whereas `schema_overrides` can be\n",
      "        used to partially overwrite a schema. Note that the order of the columns in\n",
      "        the provided `schema` must match the order of the columns in the CSV being read.\n",
      "    schema_overrides\n",
      "        Overwrite dtypes during inference; should be a {colname:dtype,} dict or,\n",
      "        if providing a list of strings to `new_columns`, a list of dtypes of\n",
      "        the same length.\n",
      "    null_values\n",
      "        Values to interpret as null values. You can provide a:\n",
      "    \n",
      "        - `str`: All values equal to this string will be null.\n",
      "        - `List[str]`: All values equal to any string in this list will be null.\n",
      "        - `Dict[str, str]`: A dictionary that maps column name to a\n",
      "          null value string.\n",
      "    \n",
      "    missing_utf8_is_empty_string\n",
      "        By default a missing value is considered to be null; if you would prefer missing\n",
      "        utf8 values to be treated as the empty string you can set this param True.\n",
      "    ignore_errors\n",
      "        Try to keep reading lines if some lines yield errors.\n",
      "        First try `infer_schema=False` to read all columns as\n",
      "        `pl.String` to check which values might cause an issue.\n",
      "    cache\n",
      "        Cache the result after reading.\n",
      "    with_column_names\n",
      "        Apply a function over the column names just in time (when they are determined);\n",
      "        this function will receive (and should return) a list of column names.\n",
      "    infer_schema\n",
      "        When `True`, the schema is inferred from the data using the first\n",
      "        `infer_schema_length` rows.\n",
      "        When `False`, the schema is not inferred and will be `pl.String` if not\n",
      "        specified in `schema` or `schema_overrides`.\n",
      "    infer_schema_length\n",
      "        The maximum number of rows to scan for schema inference.\n",
      "        If set to `None`, the full data may be scanned *(this is slow)*.\n",
      "        Set `infer_schema=False` to read all columns as `pl.String`.\n",
      "    n_rows\n",
      "        Stop reading from CSV file after reading `n_rows`.\n",
      "    encoding : {'utf8', 'utf8-lossy'}\n",
      "        Lossy means that invalid utf8 values are replaced with `�`\n",
      "        characters. Defaults to \"utf8\".\n",
      "    low_memory\n",
      "        Reduce memory pressure at the expense of performance.\n",
      "    rechunk\n",
      "        Reallocate to contiguous memory when all chunks/ files are parsed.\n",
      "    skip_rows_after_header\n",
      "        Skip this number of rows when the header is parsed.\n",
      "    row_index_name\n",
      "        If not None, this will insert a row index column with the given name into\n",
      "        the DataFrame.\n",
      "    row_index_offset\n",
      "        Offset to start the row index column (only used if the name is set).\n",
      "    try_parse_dates\n",
      "        Try to automatically parse dates. Most ISO8601-like formats\n",
      "        can be inferred, as well as a handful of others. If this does not succeed,\n",
      "        the column remains of data type `pl.String`.\n",
      "    eol_char\n",
      "        Single byte end of line character (default: `\\n`). When encountering a file\n",
      "        with windows line endings (`\\r\\n`), one can go with the default `\\n`. The extra\n",
      "        `\\r` will be removed when processed.\n",
      "    new_columns\n",
      "        Provide an explicit list of string column names to use (for example, when\n",
      "        scanning a headerless CSV file). If the given list is shorter than the width of\n",
      "        the DataFrame the remaining columns will have their original name.\n",
      "    raise_if_empty\n",
      "        When there is no data in the source, `NoDataError` is raised. If this parameter\n",
      "        is set to False, an empty LazyFrame (with no columns) is returned instead.\n",
      "    truncate_ragged_lines\n",
      "        Truncate lines that are longer than the schema.\n",
      "    decimal_comma\n",
      "        Parse floats using a comma as the decimal separator instead of a period.\n",
      "    glob\n",
      "        Expand path given via globbing rules.\n",
      "    storage_options\n",
      "        Options that indicate how to connect to a cloud provider.\n",
      "    \n",
      "        The cloud providers currently supported are AWS, GCP, and Azure.\n",
      "        See supported keys here:\n",
      "    \n",
      "        * `aws <https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html>`_\n",
      "        * `gcp <https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html>`_\n",
      "        * `azure <https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html>`_\n",
      "        * Hugging Face (`hf://`): Accepts an API key under the `token` parameter: \\\n",
      "          `{'token': '...'}`, or by setting the `HF_TOKEN` environment variable.\n",
      "    \n",
      "        If `storage_options` is not provided, Polars will try to infer the information\n",
      "        from environment variables.\n",
      "    credential_provider\n",
      "        Provide a function that can be called to provide cloud storage\n",
      "        credentials. The function is expected to return a dictionary of\n",
      "        credential keys along with an optional credential expiry time.\n",
      "    \n",
      "        .. warning::\n",
      "            This functionality is considered **unstable**. It may be changed\n",
      "            at any point without it being considered a breaking change.\n",
      "    retries\n",
      "        Number of retries if accessing a cloud instance fails.\n",
      "    file_cache_ttl\n",
      "        Amount of time to keep downloaded cloud files since their last access time,\n",
      "        in seconds. Uses the `POLARS_FILE_CACHE_TTL` environment variable\n",
      "        (which defaults to 1 hour) if not given.\n",
      "    include_file_paths\n",
      "        Include the path of the source file(s) as a column with this name.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    LazyFrame\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    read_csv : Read a CSV file into a DataFrame.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> import pathlib\n",
      "    >>>\n",
      "    >>> (\n",
      "    ...     pl.scan_csv(\"my_long_file.csv\")  # lazy, doesn't do a thing\n",
      "    ...     .select(\n",
      "    ...         [\"a\", \"c\"]\n",
      "    ...     )  # select only 2 columns (other columns will not be read)\n",
      "    ...     .filter(\n",
      "    ...         pl.col(\"a\") > 10\n",
      "    ...     )  # the filter is pushed down the scan, so less data is read into memory\n",
      "    ...     .head(100)  # constrain number of returned results to 100\n",
      "    ... )  # doctest: +SKIP\n",
      "    \n",
      "    We can use `with_column_names` to modify the header before scanning:\n",
      "    \n",
      "    >>> df = pl.DataFrame(\n",
      "    ...     {\"BrEeZaH\": [1, 2, 3, 4], \"LaNgUaGe\": [\"is\", \"hard\", \"to\", \"read\"]}\n",
      "    ... )\n",
      "    >>> path: pathlib.Path = dirpath / \"mydf.csv\"\n",
      "    >>> df.write_csv(path)\n",
      "    >>> pl.scan_csv(\n",
      "    ...     path, with_column_names=lambda cols: [col.lower() for col in cols]\n",
      "    ... ).collect()\n",
      "    shape: (4, 2)\n",
      "    ┌─────────┬──────────┐\n",
      "    │ breezah ┆ language │\n",
      "    │ ---     ┆ ---      │\n",
      "    │ i64     ┆ str      │\n",
      "    ╞═════════╪══════════╡\n",
      "    │ 1       ┆ is       │\n",
      "    │ 2       ┆ hard     │\n",
      "    │ 3       ┆ to       │\n",
      "    │ 4       ┆ read     │\n",
      "    └─────────┴──────────┘\n",
      "    \n",
      "    You can also simply replace column names (or provide them if the file has none)\n",
      "    by passing a list of new column names to the `new_columns` parameter:\n",
      "    \n",
      "    >>> df.write_csv(path)\n",
      "    >>> pl.scan_csv(\n",
      "    ...     path,\n",
      "    ...     new_columns=[\"idx\", \"txt\"],\n",
      "    ...     schema_overrides=[pl.UInt16, pl.String],\n",
      "    ... ).collect()\n",
      "    shape: (4, 2)\n",
      "    ┌─────┬──────┐\n",
      "    │ idx ┆ txt  │\n",
      "    │ --- ┆ ---  │\n",
      "    │ u16 ┆ str  │\n",
      "    ╞═════╪══════╡\n",
      "    │ 1   ┆ is   │\n",
      "    │ 2   ┆ hard │\n",
      "    │ 3   ┆ to   │\n",
      "    │ 4   ┆ read │\n",
      "    └─────┴──────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pl.scan_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447222f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "8fa48831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (33_531, 1)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>column_1</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot; 404 5021 &quot;http://www.beard.co…</td></tr><tr><td>&quot;404 4997 &quot;http://www.beard.com…</td></tr><tr><td>&quot;&quot; 404 5042 &quot;http://www.beard.c…</td></tr><tr><td>&quot; 500 5037 &quot;http://www.beard.co…</td></tr><tr><td>&quot;P/1.0&quot; 502 5095 &quot;-&quot; &quot;Mozilla/5…</td></tr><tr><td>&hellip;</td></tr><tr><td>&quot; 5026 &quot;-&quot; &quot;Mozilla/5.0 (Macint…</td></tr><tr><td>&quot;.0&quot; 303 5052 &quot;http://www.beard…</td></tr><tr><td>&quot;.0&quot; 403 5096 &quot;-&quot; &quot;Mozilla/5.0 …</td></tr><tr><td>&quot;/1.0&quot; 500 4927 &quot;-&quot; &quot;Mozilla/5.…</td></tr><tr><td>&quot;oper HTTP/1.0&quot; 502 5022 &quot;-&quot; &quot;M…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (33_531, 1)\n",
       "┌─────────────────────────────────┐\n",
       "│ column_1                        │\n",
       "│ ---                             │\n",
       "│ str                             │\n",
       "╞═════════════════════════════════╡\n",
       "│  404 5021 \"http://www.beard.co… │\n",
       "│ 404 4997 \"http://www.beard.com… │\n",
       "│ \" 404 5042 \"http://www.beard.c… │\n",
       "│  500 5037 \"http://www.beard.co… │\n",
       "│ P/1.0\" 502 5095 \"-\" \"Mozilla/5… │\n",
       "│ …                               │\n",
       "│  5026 \"-\" \"Mozilla/5.0 (Macint… │\n",
       "│ .0\" 303 5052 \"http://www.beard… │\n",
       "│ .0\" 403 5096 \"-\" \"Mozilla/5.0 … │\n",
       "│ /1.0\" 500 4927 \"-\" \"Mozilla/5.… │\n",
       "│ oper HTTP/1.0\" 502 5022 \"-\" \"M… │\n",
       "└─────────────────────────────────┘"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# improt polars\n",
    "import polars as pl\n",
    "# read the first 50 lines of the log file into a DataFrame\n",
    "df = pl.scan_csv(logfile, has_header=False, separator = '~')\n",
    "# print the DataFrame to the console\n",
    "df.collect()\n",
    "# apply regex to the DataFrame\n",
    "df.with_columns(\n",
    "    pl.col(\"column_1\").str.extract(r\"((\\d{1,3}\\.){3}\\d{1,3})\").alias(\"IP\"),\n",
    "    pl.col(\"column_1\").str.extract(r\"\\[(\\d{1,2}/\\w{3}/\\d{4})\").alias(\"date\"),\n",
    "    pl.col(\"column_1\").str.extract(r'\"(\\w*)').alias(\"method\"),\n",
    "    pl.col(\"column_1\").str.extract(r'\"\\w* (/\\w*){1,3} HTTP/1.0').alias(\"user_type\"),\n",
    "    pl.col(\"column_1\").str.extract(r'1.0\" (\\d{3})').alias(\"Status\"),\n",
    "    pl.col(\"column_1\").str.extract(r'1.0\" \\d{3} (\\d{1,4})').alias(\"port\"),\n",
    "    pl.col(\"column_1\").str.extract(r'1\\.0\"\\s\\d{3}\\s\\d+\\s\"([^\"]*)\"').alias(\"site\"),\n",
    "    pl.col(\"column_1\").str.extract(r\":(\\d{2}):\\d{2}:\\d{2}\").cast(pl.Int32).alias(\"hour\"),\n",
    "    pl.col(\"column_1\").str.extract(r\":\\d{2}:(\\d{2}):\\d{2}\").cast(pl.Int32).alias(\"minute\"),\n",
    "    pl.col(\"column_1\").str.extract(r\":\\d{2}:\\d{2}:(\\d{2})\").cast(pl.Int32).alias(\"second\"),\n",
    "    pl.col(\"column_1\").str.extract(r\"\\+(\\d{2})\\d{2}\").cast(pl.Int32).alias(\"timezone_offset\"),\n",
    ").with_columns(\n",
    "    ((pl.col(\"hour\") - pl.col(\"timezone_offset\")) % 24).alias(\"adjusted_hour\"),\n",
    "    pl.col(\"user_type\").str.strip_prefix(\"/\").alias(\"user_type\")\n",
    ").with_columns(\n",
    "    pl.concat_str([\n",
    "        pl.col(\"adjusted_hour\").cast(pl.Utf8).str.zfill(2),\n",
    "        pl.col(\"minute\").cast(pl.Utf8).str.zfill(2),\n",
    "        pl.col(\"second\").cast(pl.Utf8).str.zfill(2),\n",
    "    ], separator=\":\").alias(\"adjusted_time_string\")\n",
    ").select(\n",
    "    \"IP\", \"date\", \"adjusted_time_string\", \"method\", \"user_type\", \"Status\", \"port\",\n",
    ").collect()\n",
    "\n",
    "df.with_columns(pl.col(\"column_1\").str.slice(70)).collect()\n",
    "#df.with_columns(pl.col(\"column_1\").str.extract(r'\"\\w* (/\\w*[/, ]\\w*[/, ]\\w*)').alias(\"Method\")).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "7f8ffac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (33_531, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>column_1</th><th>site</th></tr><tr><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;73.114.209.133 - - [30/Aug/201…</td><td>&quot;http://www.beard.com/categorie…</td></tr><tr><td>&quot;223.114.129.136 - - [01/Oct/20…</td><td>&quot;http://www.beard.com/categorie…</td></tr><tr><td>&quot;136.219.6.68 - - [30/Jan/2018:…</td><td>&quot;http://www.beard.com/categorie…</td></tr><tr><td>&quot;12.234.4.43 - - [20/Mar/2019:0…</td><td>&quot;http://www.beard.com/categorie…</td></tr><tr><td>&quot;199.150.229.44 - - [30/Sep/201…</td><td>&quot;-&quot;</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;185.129.61.42 - - [18/Aug/2019…</td><td>&quot;-&quot;</td></tr><tr><td>&quot;176.31.181.203 - - [31/Jul/201…</td><td>&quot;http://www.beard.com/categorie…</td></tr><tr><td>&quot;49.247.162.253 - - [15/Aug/201…</td><td>&quot;-&quot;</td></tr><tr><td>&quot;173.83.113.31 - - [31/Jul/2018…</td><td>&quot;-&quot;</td></tr><tr><td>&quot;202.237.222.148 - - [16/Dec/20…</td><td>&quot;-&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (33_531, 2)\n",
       "┌─────────────────────────────────┬─────────────────────────────────┐\n",
       "│ column_1                        ┆ site                            │\n",
       "│ ---                             ┆ ---                             │\n",
       "│ str                             ┆ str                             │\n",
       "╞═════════════════════════════════╪═════════════════════════════════╡\n",
       "│ 73.114.209.133 - - [30/Aug/201… ┆ http://www.beard.com/categorie… │\n",
       "│ 223.114.129.136 - - [01/Oct/20… ┆ http://www.beard.com/categorie… │\n",
       "│ 136.219.6.68 - - [30/Jan/2018:… ┆ http://www.beard.com/categorie… │\n",
       "│ 12.234.4.43 - - [20/Mar/2019:0… ┆ http://www.beard.com/categorie… │\n",
       "│ 199.150.229.44 - - [30/Sep/201… ┆ -                               │\n",
       "│ …                               ┆ …                               │\n",
       "│ 185.129.61.42 - - [18/Aug/2019… ┆ -                               │\n",
       "│ 176.31.181.203 - - [31/Jul/201… ┆ http://www.beard.com/categorie… │\n",
       "│ 49.247.162.253 - - [15/Aug/201… ┆ -                               │\n",
       "│ 173.83.113.31 - - [31/Jul/2018… ┆ -                               │\n",
       "│ 202.237.222.148 - - [16/Dec/20… ┆ -                               │\n",
       "└─────────────────────────────────┴─────────────────────────────────┘"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find unique values starting from the end of r'\"(\\w*)' and ending with /1.0\n",
    "df.with_columns(pl.col(\"column_1\").str.extract(r'1\\.0\"\\s\\d{3}\\s\\d+\\s\"([^\"]*)\"').alias(\"site\")).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33239646",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = spark.read.text(logfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "023ffd40",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o100.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 1 times, most recent failure: Lost task 0.0 in stage 15.0 (TID 15) (DESKTOP-6O57JT6 executor driver): org.apache.spark.SparkRuntimeException: [INVALID_PARAMETER_VALUE.REGEX_GROUP_INDEX] The value of parameter(s) `idx` in `regexp_extract` is invalid: Expects group index between 0 and 0, but got 1.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidRegexGroupIndexError(QueryExecutionErrors.scala:357)\r\n\tat org.apache.spark.sql.catalyst.expressions.RegExpExtractBase$.checkGroupIndex(regexpExpressions.scala:735)\r\n\tat org.apache.spark.sql.catalyst.expressions.RegExpExtractBase.checkGroupIndex(regexpExpressions.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.SparkRuntimeException: [INVALID_PARAMETER_VALUE.REGEX_GROUP_INDEX] The value of parameter(s) `idx` in `regexp_extract` is invalid: Expects group index between 0 and 0, but got 1.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidRegexGroupIndexError(QueryExecutionErrors.scala:357)\r\n\tat org.apache.spark.sql.catalyst.expressions.RegExpExtractBase$.checkGroupIndex(regexpExpressions.scala:735)\r\n\tat org.apache.spark.sql.catalyst.expressions.RegExpExtractBase.checkGroupIndex(regexpExpressions.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m logs_tiny = logs.limit(\u001b[32m50\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m#logs_tiny.show(truncate=False)\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m#logs_tiny.select(regexp_extract('value', r'(\\d+\\.\\d+\\.\\d+\\.\\d+)', 1).alias('ip')).show(truncate=False)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mlogs_tiny\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregexp_extract\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvalue\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m[(\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43md\u001b[39;49m\u001b[33;43m{\u001b[39;49m\u001b[33;43m1,2}/\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[38;5;132;43;01m{3}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43md\u001b[39;49m\u001b[38;5;132;43;01m{4}\u001b[39;49;00m\u001b[33;43m)]\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tijn\\Documents\\VSC\\DAta engineering projects\\Pyspark_RegEx_practice\\.env\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:947\u001b[39m, in \u001b[36mDataFrame.show\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    887\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m = \u001b[32m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    888\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[32m    889\u001b[39m \n\u001b[32m    890\u001b[39m \u001b[33;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    945\u001b[39m \u001b[33;03m    name | Bob\u001b[39;00m\n\u001b[32m    946\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m947\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tijn\\Documents\\VSC\\DAta engineering projects\\Pyspark_RegEx_practice\\.env\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:965\u001b[39m, in \u001b[36mDataFrame._show_string\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    959\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m    960\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mNOT_BOOL\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    961\u001b[39m         message_parameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mvertical\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical).\u001b[34m__name__\u001b[39m},\n\u001b[32m    962\u001b[39m     )\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    967\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tijn\\Documents\\VSC\\DAta engineering projects\\Pyspark_RegEx_practice\\.env\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tijn\\Documents\\VSC\\DAta engineering projects\\Pyspark_RegEx_practice\\.env\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tijn\\Documents\\VSC\\DAta engineering projects\\Pyspark_RegEx_practice\\.env\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o100.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 1 times, most recent failure: Lost task 0.0 in stage 15.0 (TID 15) (DESKTOP-6O57JT6 executor driver): org.apache.spark.SparkRuntimeException: [INVALID_PARAMETER_VALUE.REGEX_GROUP_INDEX] The value of parameter(s) `idx` in `regexp_extract` is invalid: Expects group index between 0 and 0, but got 1.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidRegexGroupIndexError(QueryExecutionErrors.scala:357)\r\n\tat org.apache.spark.sql.catalyst.expressions.RegExpExtractBase$.checkGroupIndex(regexpExpressions.scala:735)\r\n\tat org.apache.spark.sql.catalyst.expressions.RegExpExtractBase.checkGroupIndex(regexpExpressions.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.SparkRuntimeException: [INVALID_PARAMETER_VALUE.REGEX_GROUP_INDEX] The value of parameter(s) `idx` in `regexp_extract` is invalid: Expects group index between 0 and 0, but got 1.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidRegexGroupIndexError(QueryExecutionErrors.scala:357)\r\n\tat org.apache.spark.sql.catalyst.expressions.RegExpExtractBase$.checkGroupIndex(regexpExpressions.scala:735)\r\n\tat org.apache.spark.sql.catalyst.expressions.RegExpExtractBase.checkGroupIndex(regexpExpressions.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract, col\n",
    "\n",
    "# take the first 50 lines and pu them in a new dataframe\n",
    "logs_tiny = logs.limit(50)\n",
    "#logs_tiny.show(truncate=False)\n",
    "#logs_tiny.select(regexp_extract('value', r'(\\d+\\.\\d+\\.\\d+\\.\\d+)', 1).alias('ip')).show(truncate=False)\n",
    "logs_tiny.select(regexp_extract('value', r'[(\\d{1,2}/\\n{3}/\\d{4})]', 1).alias('date')).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
